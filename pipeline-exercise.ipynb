{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cf4cda-c61d-4cab-ad4a-58f19b5e9c97",
   "metadata": {},
   "source": [
    "# Notebook to Dashboard converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f805c0-2984-4ed1-ac93-aa37df2c64ec",
   "metadata": {},
   "source": [
    "## Install required dependencies to work with Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd809d7-91e5-4e99-8ddd-9467052f16a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d6ac8-f71a-4164-9238-b617d6ed9227",
   "metadata": {},
   "source": [
    "## Start Ollama in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7170050-93d8-4f75-a6ac-62539c4332f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e31876-a36e-4aba-9e2c-eb6f156fb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "await module._module.load('ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4fc8dd-e25d-4526-bafe-f72c8c130fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Start a background process using subprocess.Popen\n",
    "process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9768873f-cdc5-4374-89c2-3607b12ba2cb",
   "metadata": {},
   "source": [
    "## Read source notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295f62b-dcfb-4c27-bc15-0262823a719e",
   "metadata": {},
   "source": [
    "Load the notebook here using `nbconvert`. This is the same as using `jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb` from command line. but we can do it using the underlying Python library `nbconvert`\n",
    "\n",
    "The output should look like a Python string containing all the code cells of the notebook, i.e. \n",
    "```\n",
    "'import pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n\\ndata = pd.DataFrame(np.random.randn(50, 2), columns=[\"x\",\"y\"])\\n\\nbar_data = data.copy()\\nbar_data[\\'index\\'] = bar_data.index\\nsns.barplot(x=\\'index\\', y=\\'y\\', data=bar_data)\\nplt.title(\"Bar Chart\")\\nplt.show()\\n\\n# Line plot\\nsns.lineplot(x=\\'x\\', y=\\'y\\', data=data)\\nplt.title(\"Line Chart\")\\nplt.show()\\n\\n# Fill between (this needs to be done after creating a line plot or scatter plot with Matplotlib)\\ndata = data.sort_values(\\'x\\')\\nplt.figure()  # Create a new figure\\nplt.plot(data[\\'x\\'], data[\\'y\\'], color=\\'blue\\')  # Line plot for the fill_between function\\nplt.fill_between(data[\\'x\\'], data[\\'y\\'], color=\\'blue\\', alpha=0.2)\\nplt.title(\"Area Chart\")\\nplt.show()'\n",
    "```\n",
    "\n",
    "Read documentation here: https://nbconvert.readthedocs.io/en/latest/nbconvert_library.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784ecfd-9cf7-4f8c-bf28-fa53fe24c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696e147-2638-4110-9b35-b38ae2be62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b52ca3-d8d5-40cc-a0a4-fed455e01f31",
   "metadata": {},
   "source": [
    "## Use LLM to generate dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d057ba-d803-4101-b575-094b26d5f938",
   "metadata": {},
   "source": [
    "### Set up LLM client using OpenAI librbary\n",
    "It is important to note that even though we are using OpenAI _library_, that does not mean we use OpenAI _LLM_ (GPT-4o or others) here. Following the popularity of GPT models, others implemented the API for theirs in a similar way, so that they are compatible with `openai` library. In this notebook, we are using Ollama LLM, which has a compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6adb7-2af8-4880-b5f8-dc7f64f6eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed765c-a580-41fa-ac12-c9449247909f",
   "metadata": {},
   "source": [
    "### Choose your model from the avilable models in Ollama\n",
    "\n",
    "You can check available models running `ollama list` below.\n",
    "\n",
    "Recommended models to start with are `\"llama3.2:latest\"` or `\"deepseek-coder-v2:16b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe576c4-0fc9-48a6-98fb-8356c2e56231",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c74fe-3dbd-465f-bf39-252699120f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"deepseek-coder-v2:16b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564ae09-55e5-4eff-8e76-667f09713df6",
   "metadata": {},
   "source": [
    "### Create a system prompt\n",
    "\n",
    "Refer to the Prompt Engineering course to get the idea on what to write here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a710af5-6182-4965-bd58-4271f6004ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"WRITE YOUR PROMPT HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f17845-e135-408e-8624-758a9795c67b",
   "metadata": {},
   "source": [
    "### Create a user message\n",
    "\n",
    "This should include the extracted code notebook code from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7e053-7f52-4da7-8197-05f5c4bda4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"WRITE YOUR PROMPT HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80b18d-db5c-4eb1-a577-dbb4fa68261d",
   "metadata": {},
   "source": [
    "### Make a request to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170780bd-c535-4d60-978c-c57cab6bc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=messages\n",
    ")\n",
    "\n",
    "code = response.choices[0].message.content\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17647904-e4c0-43da-a182-537359213877",
   "metadata": {},
   "source": [
    "### (Optional) Post-processing of the LLM response\n",
    "\n",
    "Some models return the code that is surrounded by triple ticks, i.e. \\`\\`\\`import pandas as pd\\`\\`\\`. \n",
    "\n",
    "This is a Markdown way to represent the block of code, so it is nicely rendered, like below\n",
    "```\n",
    "import pandas\n",
    "```\n",
    "\n",
    "However, this code won't run in Python interpreter. If this is the case, you need to write some code below to parse the `code` string and extract just the code, without triple ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b3b8e-3e88-4b61-877e-625e020ac304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e42aeb-4277-44f7-a6ef-186db9517993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87601c-eba5-4f35-8fd4-5cc38d817520",
   "metadata": {},
   "source": [
    "## Save the resulting script to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb2952-d92e-423b-848c-f83eef3f2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
